{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me a class name in wow'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt with no input\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template='give me a class name in wow')\n",
    "no_input_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me a heal class name in wow'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt with 1 input\n",
    "one_input_prompt = PromptTemplate(\n",
    "    input_variables=['game_class'], template='give me a {game_class} name in wow')\n",
    "one_input_prompt.format(game_class='heal class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me a heal class name in ff14'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt with 2 inputs\n",
    "two_input_prompt = PromptTemplate(\n",
    "    input_variables=['game_class', 'game_name'], template='give me a {game_class} name in {game_name}')\n",
    "two_input_prompt.format(game_class='heal class', game_name='ff14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game_class', 'game_name']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to get tempalte\n",
    "template = 'give me a {game_class} name in {game_name}'\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tell me the average temperature of LA at Dec 12 of 2000'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial inputs\n",
    "prompt = PromptTemplate(\n",
    "    template=\"tell me the average temperature of {city} at {date} of {year}\",\n",
    "    input_variables=['city', 'date', 'year']\n",
    ")\n",
    "partial_prompt = prompt.partial(city='LA')\n",
    "\n",
    "partial_prompt.format(year=2000, date='Dec 12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tell me the average temperature of LA at 02-15 of 2000'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you can also use function in the partial prompt\n",
    "def _get_datetime():\n",
    "    now=datetime.now()\n",
    "    return now.strftime('%m-%d')\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"tell me the average temperature of {city} at {date} of {year}\",\n",
    "    input_variables=['city', 'date', 'year']\n",
    ")\n",
    "partial_prompt = prompt.partial(city='LA', date=_get_datetime)\n",
    "partial_prompt.format(year=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### few shot: \n",
    "- providing a few examples to the model let the model know how to handle certain question (not changing the model itself)\n",
    "- fine tuning: directly modify the model itself with a lot more examples (will change the model itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are you doing?\n",
      "Greetings! Im doing well\n",
      "\n",
      "Question: what's the date today?\n",
      "Greetings! Today is Thursday\n",
      "\n",
      "Question: how is the weather now?\n",
      "Greetings! The weather is nice\n",
      "\n",
      "Question: what to learn today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"how are you doing?\",\n",
    "        \"answer\": \"Greetings! Im doing well\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what's the date today?\",\n",
    "        \"answer\": \"Greetings! Today is Thursday\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"how is the weather now?\",\n",
    "        \"answer\": \"Greetings! The weather is nice\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# apply the prompt template to the examples above\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['question', 'answer'],\n",
    "    template=\"Question: {question}\\n{answer}\"\n",
    "    )\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"what to learn today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n1. A new language: Learning a new language can open up opportunities for travel, career advancement, and personal enrichment.\\n\\n2. Coding/programming: In today's digital age, having coding skills can be highly beneficial for job prospects and understanding technology.\\n\\n3. Photography: Learning how to take great photos can help you capture memories and express your creativity.\\n\\n4. Cooking/baking: Learning new recipes and cooking techniques can improve your health, save you money, and impress your friends and family.\\n\\n5. Personal finance: Understanding how to manage your money, invest, and save for the future is a valuable life skill.\\n\\n6. Meditation/mindfulness: Practicing mindfulness and meditation can reduce stress, improve focus, and promote overall well-being.\\n\\n7. First aid/CPR: Knowing basic first aid and CPR can potentially save a life in an emergency situation.\\n\\n8. Graphic design: Learning graphic design skills can help you create visually appealing content for personal or professional use.\\n\\n9. Musical instrument: Learning to play a musical instrument can improve cognitive skills, relieve stress, and provide a creative outlet.\\n\\n10. Public speaking: Developing public speaking skills can boost confidence and improve communication in various aspects of life.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('Question: what to learn today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreetings! That depends on your interests and goals. What subjects or skills are you interested in learning?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, the key difference is the \"greetings\" at the beginning of the response\n",
    "llm.predict(prompt.format(input=\"what to learn today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExampleSelector\n",
    "- when you have a lot of examples, you could pass selected examples to few shot based on certain criteria (such as similarity)\n",
    "- it can also reduce the number of tokens sent to LLM, which could help reduce cost\n",
    "- Example: SemanticSimilarityExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # The number of examples to produce.\n",
    "    k=1,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Input is a feeling, so should select the happy/sad example\n",
    "print(similar_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: large\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Input is a measurement, so should select the tall/short example\n",
    "print(similar_prompt.format(adjective=\"large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: enthusiastic\n",
      "Output: apathetic\n",
      "\n",
      "Input: passionate\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# You can add new examples to the SemanticSimilarityExampleSelector as well\n",
    "similar_prompt.example_selector.add_example(\n",
    "    {\"input\": \"enthusiastic\", \"output\": \"apathetic\"}\n",
    ")\n",
    "print(similar_prompt.format(adjective=\"passionate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm module\n",
    "- predict(): direct call for 1 input\n",
    "- generate(): batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('tell me a joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.generate(['tell me a joke', 'whos the best player in NBA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=\"\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe best player in the NBA is currently debatable, as it is subjective and can vary based on individual opinions. Some people may argue that LeBron James, Kevin Durant, or Kawhi Leonard are the best players, while others may argue for players like Giannis Antetokounmpo, Stephen Curry, or James Harden. Ultimately, it is up to personal preference and can change over time.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 110, 'prompt_tokens': 11, 'completion_tokens': 99}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('4cb0341b-4de7-4792-a6c1-3e81e136c47a')), RunInfo(run_id=UUID('e8959017-c3e7-45e9-9b9b-467b1a8d991d'))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe best player in the NBA is currently debatable, as it is subjective and can vary based on individual opinions. Some people may argue that LeBron James, Kevin Durant, or Kawhi Leonard are the best players, while others may argue for players like Giannis Antetokounmpo, Stephen Curry, or James Harden. Ultimately, it is up to personal preference and can change over time.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[1][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customize LLM module\n",
    "- to wrap the new models that are not supported in langchain \n",
    "- to create similation tests\n",
    "- to define when certain inputs are provided, how the output would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentAI(LLM):\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"SilentAI\"\n",
    "    \n",
    "    def _call(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted\")\n",
    "        pd = prompt.find(\"?\")\n",
    "        if pd >= 0:\n",
    "            return prompt[0:pd] + \" o7\"\n",
    "        return \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSilentAI\u001b[0m\n",
      "Params: {}\n"
     ]
    }
   ],
   "source": [
    "cllm = SilentAI()\n",
    "print(cllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are you o7'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cllm.predict(\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how is food o7'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cllm.predict(\"how is food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cllm.predict(\"i dont like your response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
