{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic chain structure\n",
    "- LLM chain: \n",
    "    - define prompt\n",
    "    - define llm\n",
    "    - define chain\n",
    "    - run chain.predict()\n",
    "- Router chain:\n",
    "    - given different expert chains, decide which expert chain, then return the results\n",
    "    - define multiple prompts\n",
    "    - define multiple llm/embeddings\n",
    "    - define chain (multiple chains at the same time)\n",
    "    - run predict\n",
    "- Sequential chain\n",
    "    - the output from previous step becomes the input for the next step\n",
    "    - define multiple prompts\n",
    "    - define multiple llm/embeddings\n",
    "    - define chain (multiple chains sequentially)\n",
    "    - run predict\n",
    "- Transformation chain\n",
    "    - text processing chain to normalize the inputs\n",
    "\n",
    "### chain application examples (typical usage structure)\n",
    "- Document chains\n",
    "    - used specifically for long text processing (such as summary, vectorDB)\n",
    "    - Stuff\n",
    "        - for a given question, pass the question and context together to the LLM\n",
    "        - however, in many cases, it would exceed the upper limit of the prompt length for LLMs\n",
    "    - Refine\n",
    "        - for a given question, pass the question and sliced context to the LLM and give an intermediate answer\n",
    "        - then, the intermediate answer is passed togehther with question and sliced context to the LLM\n",
    "        - in this way, it would help to avoid the upper prompt length limit for LLMs\n",
    "    - Map reduce\n",
    "    - Map rank\n",
    "- Retrieval QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Koko\" '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "prompt_template = \"give me a name for a cute Siamese cat\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_template= '''\n",
    "You are a pet store owner in Japan. \n",
    "You are good at naming pets with good Japanese names.\n",
    "\n",
    "here is the question {input}\n",
    "'''\n",
    "\n",
    "math_template = '''\n",
    "You are a good mathematician. You are great at answering math probrams.\n",
    "You are so good because you are able to break down hard problems into their component parts, \n",
    "answer the component parts, and then put them together to answer the broader question\n",
    "\n",
    "here is the question {input}\n",
    "'''\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"naming\",\n",
    "        \"description\": \"good for making Japanese name for pet\",\n",
    "        \"prompt_template\": naming_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"good for answering math questions\",\n",
    "        \"prompt_template\": math_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.router import MultiPromptChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inidividual chain\n",
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info['name']\n",
    "    prompt_template = p_info['prompt_template']\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=['input']\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "default_chain = ConversationChain(llm=llm, output_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables=['input'],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\billl\\miniconda3\\envs\\llm_proj\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naming: {'input': 'give me a name for a cute Siamese cat'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\"Kojiro\" (小次郎)\n"
     ]
    }
   ],
   "source": [
    "# it first find the naming chain\n",
    "# then run the chain in the naming branch\n",
    "print(chain.run(\"give me a name for a cute Siamese cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\billl\\miniconda3\\envs\\llm_proj\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math: {'input': '\\n    if A and B are 100mile away from each other and are traveling twowards each other.\\n    A travels at 50mile per hour, while B travels at 25miles per hour.\\n    How long does it take A and B to meet each other?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "To solve this problem, we can use the formula d = rt, where d is the distance, r is the rate, and t is the time. Since A and B are traveling towards each other, their distances will add up to 100 miles.\n",
      "\n",
      "So, for A, d = 50t and for B, d = 25t. Setting these two equations equal to each other, we get 50t = 25t, or t = 2 hours.\n",
      "\n",
      "Therefore, it will take A and B 2 hours to meet each other. This is because in 2 hours, A would have traveled 50 miles and B would have traveled 25 miles, adding up to 100 miles and meeting at the middle point.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"\"\"\n",
    "                if A and B are 100mile away from each other and are traveling twowards each other.\n",
    "                A travels at 50mile per hour, while B travels at 25miles per hour.\n",
    "                How long does it take A and B to meet each other?\n",
    "                \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template= '''\n",
    "You are a pet store owner in Japan. \n",
    "You are good at naming pets with good Japanese names.\n",
    "Please give three names\n",
    "\n",
    "here is the question {input}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template= '''\n",
    "You are a pet name evaluator\n",
    "You are good at analyzing pet names and decide which one is better\n",
    "\n",
    "Pet name analysis:\n",
    "{synopsis}\n",
    "Analyze the above names and decide which one is better\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"synopsis\"])\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains = [synopsis_chain, review_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "1. Sakura (meaning \"cherry blossom\")\n",
      "2. Hikaru (meaning \"shining\" or \"radiant\")\n",
      "3. Kaida (meaning \"little dragon\")\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Based on meaning and uniqueness, I would say Sakura is the better pet name. It has a beautiful and meaningful association with the cherry blossom, and it is not a very common pet name. Hikaru is also a nice name, but it is more commonly used in Japan as a human name. Kaida is a cute name, but it may not have as much significance or depth as the other two names. Overall, Sakura would make a great pet name for its beauty and symbolism.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"give me a name for a cute Siamese cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document chain type (long text processing chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
